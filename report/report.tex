\documentclass[journal=jacsat, manuscript=article]{achemso}

\usepackage[version=3]{mhchem}
\usepackage{courier}
\usepackage{comment}
\usepackage{hyperref}

\newcommand*{\mycommand}[1]{\texttt{\emph{#1}}}

\author{Sabrina Jiang}
\author{Will Sherwood}
\author{Kasra Sadeghi}

\email{}

\title{Backbone Compiler}

\begin{document}

\begin{abstract}

The goal of this project is two-fold: firstly, design a programming language that has enough features
to run system calls and do basic programming tasks. Secondly, the programming language should be designed with the intent of it being able to compile itself.
We completed the first task by creating Backbone, a lisp-like language that has recursive functions, let bindings, arithmetic and more basic features. 
To complete the second task, we wrote a compiler for Backbone in C to translate it directly into LLVM bytecode. This was created in a way to make it easy to translate into Backbone later, once the language had been developed more. This would result in a self-bootstrapping compiler in Backbone.

\end{abstract}

\section{Introduction}

Our language, Backbone, was designed to be able to eventually compile itself. We worked towards this goal by first writing a C compiler in a way that allowed us to easily translate into Backbone later. This meant that at times, we couldn't use convenient C shortcuts to write the compiler, since it was likely that Backbone wouldn't be able to support such complex features.

Despite failing to complete the Backbone compiler written in Backbone for this project, we were able to flesh out a parser, LLVM generator, and more in the amount of time we had. Additionally, having these written in C was an advantage because we wanted Backbone to be inspired in part by C.

We chose to compile Backbone into LLVM, a widely used low-level language that was created to be flexible and reliable. LLVM started as a research project and has now become a library of modular technologies intended for use in a compiler, perfect for our needs in this project. Also, we use Clang and CMake to compile our source code, both of which are respectable C compilers. Clang in particular was convenient because it is an LLVM - native compiler.

The rest of this paper will spend its time detailing the design decisions made for Backbone. It will cover its features, syntax, and compiler semantics. There are code examples sprinkled throughout to help show how Backbone looks and feels.

\section{Language}

The motivation behind the design of the Backbone language was to be
simple enough to write a simple compiler for it, but also have enough
features to be able to write a backbone compiler in the backbone language.
Through many design choices, we came to the language syntax detailed below. 

After some trial and error, we concluded that the best way to make syntax decisions was to first write the target feature in LLVM, examine the syntax, and then reverse-engineer the corresponding Backbone syntax. For example, while creating function definitions, the original syntax was as follows: 
$$\texttt{(def main (paramlist (param argc i32) (param argv i8**))}$$

However, after examining the LLVM and discussing which keywords were necessary, we decided on the following format instead:
$$\texttt{(def main (params (argc i32) (argv i8**)) i32}$$

This new format was much better. Firstly, it provides everything needed to generate the LLVM. We realized the return type was required knowledge, and added that to the definition. Secondly, it eliminates some of the unnecessary keywords.

The rest of our syntax was decided in a similar manner. The biggest consideration was balancing between inferring information, such as types of identifiers and arguments, and requiring the programmer to explicitly state the types. In the end, we generally decided that asking the programmer for more information was a better choice, as it made our jobs a bit easier. This helped eliminate the need for multiple passes through the program.

\section{Flattening Compiler Pass}

\subsection{Overview}

Flatten is unique in that it is the only transformation from a symbolic expression to another symbolic expression. It is unlike the
other phases, Parser and Gen, in that it is an
\href{https://en.wikipedia.org/wiki/Operator_(mathematics)}{operator}. This makes it a pure
transformation of trees, and it is how we implement compiler passes.

The structure of our language, compared to the LLVM Intermediate Representation language
(LLVM IR), gives more flexibility to the expression-value relationship. In LLVM IR,
there is one expression evaluation per instruction, as instruction evaluation is
thought of as an opcode. In our language, we generalize the strictness of the LLVM IR
in order to allow for a more natural expression of ideas and a way to elide the needless
use of extra variable names and register management.

In our language, we separate Expressions that are immediate Values or Expressions that
can have Expressions inside of them, which we call Tall Expressions (also Unflat Expressions).

$$\texttt{Expression = Value | Tall}$$

In LLVM IR, there are
\href{https://llvm.org/docs/LangRef.html#first-class-types}{first-class types}.
These correspond to our language's Value Expressions. It often requires that the arguments
to an opcode be first-class types. For example, every argument in the argument list in a
function call instruction must be a first-class type. Our language defines call's arguments
as expressions, and then it flattens each argument (see \hyperref[sec:Flattening a Tall Expression]{Flattening a Tall Expression})
if it is not an immediate value.

\subsection{Flattening the Program}

Flattening a program basically boils down to flattening each of it's blocks, which
correspond to LLVM control blocks. Each of Backbone's blocks must end in a return, while
in LLVM there are other kinds of terminal instructions.

\subsection{Flattening a Tall Expression}

Flattening a Tall Expression extracts the expression and places it in a let. To create a
value for the let, we have a global counter for the whole flatten call. Local variable
names only have to be unique in each function definition, so we reset the counter each
time we flatten a function definition. Once we extract the let, we insert the let before
the statement we extracted it from with the name received from the local counter (e.g. $\$5$).

To keep invariants consistent, the size of the encompassing block must be increased by one,
and with the introduction of if-statements we have recursive search through blocks, so we
have to keep track of the current block and cache it every time we recurse inwards
(nBlock).

Once we've inserted the let statement, we also need to recurse into it to make sure that
the expression we just flattened does not need further flattening 
(see \hyperref[sec:Flattening Let Statements]{Flattening Let Statements}).

\subsection{Flattening Let Statements}

Flattening let statements is where we do the bulk of the initial investigation into flattening
expressions. We check to see if the child of each let is one of the expressions that can have
expression children, i.e. they are Tall. This is also where we do \hyperref[sec:Call-as-a-Statement]{Call-as-a-Statement}.

\subsection{Call-as-a-Statement}

Flatten also has a Call-as-a-Statement call as it traverses through statements as a
convenience. Sometimes we call a function and we want to ignore the result. So we just
place the call into a let, replace it, and use the local counter that normalize has available to
it to ease the work.

\section{Parser}

\subsection{Overview}

The parser of most languages is fairly tedious to write and a great deal of research has
gone into writing efficient parsers. There's a great number of parsing algorithms to choose
from, but most big projects roll their own parser or use a parser-lexer framework like
flex/bison (or lex/yacc). We don't have a back-end for any of the frameworks because our
language doesn't exist yet, so it's a better idea to write a very simple parser for a language
with very simple structure.

It would be a tremendous time investment to write a parser for our language in C and then write
it again in our language if we were to choose a complex structure for the parser. There are two
language structures that are fairly simple to parse. Concatenative languages like Forth or Factor
have a very simple structure. Each token is separated by a space, and the evaluation of the
language is token by token, in-order (most of the time). The other choice is a LISP-like language.
We chose to use a LISP-like language structure because it is more natural to express ideas in that
are already structured like a tree, and our bootstrapping language is C which has a tree-based
structure. It also doesn't allow for ambiguous parses, because there is no order of operations
in a LISP (there are more than enough parentheses to clarify).

\subsection{Overlying Decisions}

If you look in our examples/ directory, you can see the code samples we thought about to make
decisions about the structure of the language. We grounded on the following rules:
\begin{enumerate}
\item our generator is not going to be very interesting
\item we're not going to do any optimization in the first phase.
\item the syntax for the language must align closely with the information available in LLVM IR
\item there should usually be a one to one correspondence, or a very simple lookup in the tree.
\item names are more important than types, so types should follow the names.
\end{enumerate}

\subsection{Divergence from Lisp}

Our language looks like a LISP, but only for the easy parsing. We don't have any Lisp features
like homoiconicity, hygienic macros, garbage collection, etc. We also don't obey the traditional
LISP definition of a list. In LISP:

$\texttt{S-expression = List | Atom}$

$\texttt{List = ( Sexp* )}$

However, in Backbone, we have:

$\texttt{S-expression = List | Atom}$

$\texttt{List = ( Word Sexp* )}$

$\texttt{Atom = Word | String | Char}$

\subsection{Future Plans}

Verify the characters allowed in different expressions and allow for better error reporting for
incorrect syntax. Validate the resulting tree and make sure it is a some minimal amount of
a valid Backbone program instead of crashing or generating invalid LLVM. These were not
prioritized because making the language Turing complete and capable of bootstrapping itself
were prioritized during the first phase.

\section{Backbone}

\subsection{Language terminals}

A few language terminals will be described here and used in the rest of the
language to construct most of the other features.

\begin{align}
    id &\rightarrow \textt{[a-zA-Z\_][a-zA-Z0-9\_]}\,\,\,\,\textt{(identifiers)}\\
    type &\rightarrow (i8|i16|i32|i64|u8|u16|u32|u64)*^*\,\,\,\,\textt{(types)} \\
\end{align}

\subsection{Expressions and Statements}

There are two kinds of code execution in Backbone: Statements and expressions.
Statements are a standalone piece of execution that executes only for its side effects.
Specifically, input, output, and let bindings are the most useful applications of
statements in Backbone. Expressions, on the other hand, are evaluated for their use in 
statements. Expressions simply reduce to values at runtime, and live to give meaning
to the side effects produced by statements.

Specifically, an "Expression", which will be used in the grammar definitions of
many structures throughout backbone, can be any of the following: A $\texttt{call}$, $\texttt{call-vargs}$, an arithmetic operator such as $+$ or $-$,
a comparison, a $\texttt{load}$, or itself just a value.

Statements, on the other hand, have a variety of meanings: Statements include let bindings,
return statements, conditional evaluations, calls and call-vargs, automatically deleted
declarations, and storing a value into a variable. Notice that both variadic and regular
calls are statements and expressions. The reason for this is that sometimes the value is not
needed for a function, and it was a nice language feature to be able to simply call a function
without having to use a let binding with an ignored variable name.

\subsection{Structs}

Users can define their own struct data types as a statement. The general
grammar is defined as follows:

\begin{align}
Struct &\rightarrow (\texttt{struct}\,id\,Fields) \\
Fields &\rightarrow Field\,Fields\,|\,Field \\
Field  &\rightarrow (id\,type)
\end{align}

An example of this syntax would be the following struct definition:

$\texttt{(struct SymbolicExpression}$

$\texttt{  (value i8*)}$

$\texttt{  (list SymbolicExpression**)}$

$\texttt{  (len u64)}$

$\texttt{  (cap u64))}$

Structs can be indexed using the $\texttt{(index} id type \texttt{Expression)}$ expression. Here, the
id is the identifier for the struct, type is the definition of the struct, and Expression is an expression
that evaluates to a $\texttt{i32}$ value. This expression returns the value that is stored inside the
struct given the specific index. In the future, we would like to support named variables in structs which
would allow for more code readability, but would not add to functionality.

Since a pointer to an element in a struct can be retrieved using $\textt{index}$, values can easily be
modified by using the $\texttt{store}$ keyword. Likewise, an element inside of a struct can be retrieved
using the $\texttt{load}$ keyword after indexing. The following is an example of the use of structs, followed
by the corresponding LLVM genereated by the compiler:

\footnotesize{ 
$\texttt{(struct Basic}$

$\texttt{  (a i32))}$

$\texttt{}$

$\texttt{(str-table}$

$\texttt{  (0 "Basic\{a = \%d\}\\0A\\00"))}$

$\texttt{}$

$\texttt{(decl calloc (types i64 i64) i8*)}$

$\texttt{}$

$\texttt{(def makeBasic (params (a i32)) Basic*}$

$\texttt{  (let r (cast i8* Basic* (call calloc (types i64 i64) i8* (args 1 8))))}$

$\texttt{  (store a i32 (index r Basic 0))}$

$\texttt{  (return r Basic*))}$

$\texttt{}$

$\texttt{(decl printf (types i8* ...) i32)}$

$\texttt{}$

$\texttt{(def main (params (argc i32) (argv i8**)) i32}$

$\texttt{  (let t (call makeBasic (types i32) Basic* (args 7)))}$

$\texttt{  (call-vargs printf (types i8* i32) i32 (args}$

$\texttt{    (str-get 0)}$

$\texttt{    (load i32 (index t Basic 0))))}$

$\texttt{  (return 0 i32))}$
}

\normalsize
And the corresponding LLVM bytecode:

\footnotesize{
$\texttt{\%struct.Basic = type \{ i32 \}}$

$\texttt{@str.0 = private unnamed\_addr constant [15 x i8] c"Basic\{a = \%d\}\\0A\\00", align 1}$

$\texttt{}$

$\texttt{declare i8* @calloc(i64, i64)}$

$\texttt{declare i32 @printf(i8*, ...)}$

$\texttt{define \%struct.Basic* @makeBasic(i32 \%a) \{}$

$\texttt{entry:}$
  
$\texttt{  \%\$0 = call i8* (i64, i64) @calloc(i64 1, i64 8)}$
  
$\texttt{  \%r = bitcast i8* \%\$0 to \%struct.Basic*}$
  
$\texttt{  \%\$1 = getelementptr inbounds \%struct.Basic, \%struct.Basic* \%r, i32 0, i32 0}$
  
$\texttt{  store i32 \%a, i32* \%\$1}$
  
$\texttt{  ret \%struct.Basic* \%r}$

$\texttt{\}}$

$\texttt{}$

$\texttt{define i32 @main(i32 \%argc, i8** \%argv) \{}$

$\texttt{entry:}$
  
$\texttt{  \%t = call \%struct.Basic* (i32) @makeBasic(i32 7)}$
  
$\texttt{  \%\$2 = getelementptr inbounds \%struct.Basic, \%struct.Basic* \%t, i32 0, i32 0}$
  
$\texttt{  \%\$1 = load i32, i32* \%\$2}$
  
$\texttt{  \%\$0 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([15 x i8], [15 x i8]* @str.0, i64 0, i64 0),
i32 \%\$1)}$

$\texttt{  ret i32 0}$

$\texttt{\}}$
}

\normalsize

Which has the expected output of $\texttt{Basic\{a = 7\}}$.

\subsection{Calls}

Backbone supports statically typed functions that can be recursive and nested. Splitting code into
subroutines or functions is incredibly important in software engineering, and are vital to effectively
implementing system calls.

The general grammar is as follows:

$$\texttt{call}\,\,\,id\,\,\,(\texttt{types}\,type^*)\,\,\,(\texttt{args}\,\,\,Expression^*)\,\,\,type$$

Call is one of the more interesting features since it is both a statement and an expression.
The implications of this is that we can have nested calls. Since LLVM requires one expression evaluation
per instruction, calls with nested expressions need to be evaluated one at a time. We do this by
making a pass over the syntax tree and flattening expressions. This is further discussed in our section
on flattening.

An example of this syntax would be the following call statement:

$\texttt{(call printSexp void ((program Sexp*) (0 i32)))}$

\subsection{Qualified Types}

Converting between backbone and LLVM types is fairly simple. If the type is a user defined struct,
the LLVM type replaced is simply $\%struct.$ concatenated with the user defined struct type. If it
is a primitive type, or a pointer to a primitive type, they have the same representation. Here
are two examples of qualified types:

\begin{align}
\texttt{i32***}\quad &\rightarrow \quad \texttt{i32***} \\
\texttt{SymbolicExpression**}\quad &\rightarrow\quad \texttt{\%struct.SymbolicExpression**}
\end{align}

\subsection{Let}


\subsection{Conditionals}

One of the most basic functionalities of programming languages is the ability of the program to branch, typically completed with some sort of condition-checking. Backbone, like many other languages, uses the \texttt{if} keyword to indicate this. After the \texttt{if} keyword, the programmer should have a predicate, and then an expression that should be evaluated if the predicate evaluates to a non-zero number.

The general grammar is as follows:

$\texttt{if (predicate)}$

$\texttt{    (statement}^+\texttt{)}$

The following syntax example has a predicates that compare two numbers, the syntax of which will described shortly:

$\texttt{(if (< i32 2 1)}$

$\texttt{     (call puts (types i8*) i32 (args (str-get 0))))}$

$\texttt{(return 0 i32)}$

\subsection{Return}

Of course, it is important to be able to return from a function call with useful information, so that the result of the function can be used elsewhere.
Our return simply requires the programmer to use the \texttt{return} keyword, followed by the value to return and then its return type. This return type should match the return type of the function that was declared.
The general grammar is as follows:

$\texttt{return expression}\quad type$

An example of this syntax would be the following:

$\texttt{(return i i32)}$

\subsection{Declarations}

We support declaration statements similar to C with the following syntax:

$\texttt{(decl id (types type}^* \texttt{) type)}$

An example of a declaration would be the "puts" system call:

$\texttt{(decl puts (types i8*) i32)}$

\subsection{Function Definitions}

As mentioned, Backbone supports function definitions, useful for separating code into reusable chunks and making programming similar tasks much simpler. Despite being a more challenging feature to support, we all agreed that functions were completely necessary for this project. 

Firstly, to define the name, return type, and arguments of the function, the programmer should use the \texttt{def} keyword, followed by the name of the function. Next, the parameters should be listed after a \texttt{params} keyword, each with the id and type of the argument. The return type is then added on after, and finally, the body of the function. This should end in a return statement, such that the return statement's type matches the return type of the function definitions.
The general syntax is as follows:

$\texttt{(def }\,\,\,id\,\,\,\texttt{(params (}\,\,\,id\,\,\,\,\,\,type\,\,\,\texttt{))}\,\,\,type\,\,\,\texttt{(statement}^+\texttt{))}$


The following syntax creates a function called \texttt{func} with two arguments, \texttt{a} of type 32-bit int, and \texttt{b} also of type 32-bit int. \texttt{func} simply returns the value of its argument a:

$\texttt{(def func (params (a i32) (b i32)) i32}$

$\texttt{  (return a i32))}$

\subsection{Binary Operations} 

Backbone supports a number of binary operations, including arithmetic and comparisons.

\subsubsection{Comparators}

As mentioned above, the ability to compare two different values is essential for conditionals and other basic calculations. Our comparator syntax has four parts.

The first part is the desired operation, <, <=, >, or >=. Additionally, the type must be specified, such as a 32-bit integer. Finally, the two expressions whose values are to be compared.

The general grammar is as follows:

$$\,\,\,comparator\,\,\,\,\,\,type\,\,\, \texttt{expression } \texttt{expression}$$

Some simple examples are below: 

$\texttt{(< i32 3 2)}$

$\texttt{(< i32 x y)}$

\subsubsection{Arithmetic}



$\texttt{}$

$\texttt{(+ i32 2 3)}$

\section{Samples}

This section contains some simple examples that we used for unit testing, and are great for showcasing the full process.

Here is an example Backbone program. It simply calls a function to print "Hello World", but we are more interested in the LLVM it generates.

$\texttt{(str-table}$

$\texttt{  (0 "Hello World\textbackslash00"))}$


$\texttt{(decl puts (types i8*) i32)}$


$\texttt{(def voidcall (params) voi}$

$\texttt{  (call puts (types i8*) i32 (args (str-get 0)))}$

$\texttt{  (return void))}$


$\texttt{(def main (params) i32}$

$\texttt{  (call voidcall (types) void (args))}$

$\texttt{  (return 0 i32))}$

$\texttt{}$


The corresponding LLVM that would be generated is below. Notice how named functions and constants are directly passed down into the LLVM:

$\texttt{}$


$\texttt{; ModuleID = 'voidcall.bb'}$

$\texttt{target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"}$

$\texttt{target triple = "x86\_64-unknown-linux-gnu"}$

$\texttt{@str.0 = private unnamed\_addr constant [12 x i8] c"Hello World\textbackslash00",}$

$\texttt{align 1}$

$\texttt{declare i32 @puts(i8*)}$

$\texttt{define void @voidcall() \{}$

$\texttt{entry:}$

$\texttt{  \%\$0 = call i32 (i8*) @puts(i8* getelementptr inbounds ([12 x i8], }$

$\texttt{  [12 x i8]* \@str.0, i64 0, i64 0)) }$

$\texttt{  ret void}$

$\texttt{\}}$

$\texttt{define i32 @main() \{}$

$\texttt{entry:}$

$\texttt{  call void () @voidcall()}$

$\texttt{  ret i32 0}$

$\texttt{\}}$

\section{Conclusion}

In conclusion, overall the language design was a success. Sadly we were not able to fully construct
another version of the compiler written itself in backbone, but the C compiler is fully functional
and should be a relatively simple transpose as a future work. Some of the harder problems that were overcome
were tree flattening which wrote nested calls and returns as single evaluations, automatic variable
deletion from stack, as well as defining useful data types such as structs that are interoperable
with C and Linux system calls. In the future, it would be good to have some formal verification of the
language (writing and checking semantics in a proof checker), and type inference. All in all, however,
the language is usable and works to create full interoperability between calling C and Backbone functions
as well as have it compile to a low level intermediate representation language (LLVM).

\end{document}
